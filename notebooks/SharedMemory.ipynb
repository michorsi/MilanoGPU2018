{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(6): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    // First we stride through global memory and compute the maximum for every thread.\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;   // blockIdx.x is always zero because we use just one block\n",
    "    \n",
    "    float max_value = -9999999.99;   // FIX ME\n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Store the pre-thread maximum in shared memory.\n",
    "    __shared__ float max_shared[64];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Synchronize so that all threads see the same shared memory.\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the maximum in shared memory.\n",
    "    // Reduce from 64 to 32 elements\n",
    "    if (threadIdx.x < 32) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 32]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // Reduce from 32 to 16 elements\n",
    "    if (threadIdx.x < 16) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // Reduce from 16 to 8 elements\n",
    "    if (threadIdx.x < 8) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 8]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // Reduce from 8 to 4 elements\n",
    "    if (threadIdx.x < 4) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 4]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // Reduce from 4 to 2 elements\n",
    "    if (threadIdx.x < 2) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 2]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    // Reduce from 2 to 1 element\n",
    "    if (threadIdx.x < 1) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 1]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Finally write out to output.\n",
    "    if (threadIdx.x == 0) {\n",
    "        output[0] = max_shared[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "a = np.random.random((1, n)).astype(np.float32)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 64\n",
    "b = np.empty((1, num_threads), dtype=np.float32)\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8652245  0.6028568  0.49051866 0.37550572 0.30786648 0.21346712\n",
      "  0.24728757 0.8001967  0.7523666  0.18181866 0.93468153 0.77047676\n",
      "  0.74495023 0.9404862  0.00633006 0.29288366 0.36989376 0.95670927\n",
      "  0.2265881  0.50838155 0.5947812  0.38475344 0.1556996  0.82997346\n",
      "  0.9160598  0.99135363 0.69199544 0.4045     0.8296572  0.5686859\n",
      "  0.16014126 0.8327087  0.5546283  0.17468268 0.7989543  0.7365818\n",
      "  0.85785705 0.8810847  0.8704667  0.9980439  0.8701734  0.00577471\n",
      "  0.7038801  0.6449104  0.15989287 0.81345356 0.5683608  0.57394755\n",
      "  0.90552723 0.36622635 0.1664252  0.18958624 0.986221   0.9413474\n",
      "  0.14121401 0.74505246 0.82000947 0.13585423 0.2565578  0.01533947\n",
      "  0.929867   0.86964154 0.9166347  0.39545536]]\n",
      "[[0.9980439  0.9684571  0.33426452 0.19167586 0.12459923 0.5402504\n",
      "  0.34930632 0.2662844  0.14671509 0.78913593 0.71126646 0.23983198\n",
      "  0.3289858  0.8386073  0.22462337 0.52970004 0.22329433 0.13470535\n",
      "  0.87593037 0.12188934 0.70764905 0.81207794 0.75761294 0.41417593\n",
      "  0.44012612 0.05569275 0.335405   0.51574546 0.4577629  0.23383978\n",
      "  0.6349246  0.4651166  0.51925474 0.1976909  0.5412956  0.19103758\n",
      "  0.8644726  0.83212215 0.25031322 0.48525664 0.5370773  0.97222376\n",
      "  0.6164777  0.57448083 0.7726863  0.7998707  0.37801355 0.4002809\n",
      "  0.6958001  0.7914533  0.41140205 0.26763868 0.903664   0.48471183\n",
      "  0.613224   0.9237154  0.02956653 0.5826872  0.5767226  0.26506916\n",
      "  0.09905683 0.13391691 0.28695527 0.53231484]]\n",
      "0.9980439\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), grid=grid_size, block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
